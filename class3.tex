\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\chapter{Lecture-3 May, 25, 2021}

\section{Review from the Last Lecture}
We have learned Perceptron and it's learning strategy:
\begin{itemize}
    \item Activation function \textbf{MUST} be Hard-Limiter [0,1]; \textcolor{red}{WHY???}
    \item Learningis: $\mathbb{W}_{}^{new} = {\mathbb{W}^{old}} + (\mathbb{t} - \mathbb{y}){\mathbb{X}^{*T}} = {\mathbb{W}^{old}} + {\mathbb{e}_{}}\mathbb{X}_{}^{*T}$, for both weights and threshold;
\end{itemize}
\noindent{\color{red} \rule{\linewidth}{0.5mm}}
% ====================================================================================
\section{Adaline and Madaline}
Today, we are going to look at Adaline and Madaline:
\begin{itemize}
    \item \textbf{Adaline} stands for \textbf{Ada}ptive \textbf{Line}ar Neuron Network;\textcolor{red}{What the word linear means here as the activation function can be non-linear???}
    \item \textbf{Madaline} stands for \textbf{M}any \textbf{Adaline} in Parallel; Mainly used for communication, DSP, Control, and Robotics.
\end{itemize}
These concepts were introduced by Barnard Widraw and Marcian Hoff in the 60s, together with LMS (least mean square) training algorithms. 

\noindent{\color{red} \rule{\linewidth}{0.5mm}}
% ====================================================================================
\section{Single Neuron Adaline Model}

For a \textcolor{red}{fully connected} single neuron Adaline neural network, which has \textcolor{red}{one neurons (one outputs) and N inputs},
for the 1st neuron, we have:
\[{y} = f(\sum\limits_{i = 1}^N {{w_{i}}} {x_i} - {\theta})\]
If we choose $ - {\theta _j} = {x_0} {w_{j0}}$:  i.e. $x_0 \equiv 1$, and then, $w_{j0}\equiv-\theta_0$
Then, the argument-ed y becomes:
\[{y} = f(\sum\limits_{i = 0}^N {{w_{i}}} {x_i})\]
Where i could be any number between 0 to N:
\begin{itemize}
    \item $w_i$ is the weight from the ith input to the neuron,
    \item $i = 0$ is the term for the threshold, where $w_0 = - \theta$
\end{itemize}
All these $w_i$ are randomly initialized to $[-1,+1]$.


\begin{center}
\begin{tikzpicture}
\Vertex[x=0, RGB,color={190,174,212},label=\(x_0\)]{A} 
% Vertexs
\Vertex[x=2,label=$x_1$]{B}
\Vertex[x=4,label=$...$]{C}
\Vertex[x=6,label=$x_i$]{D}
\Vertex[x=8,label=$...$]{D1}
\Vertex[x=10,label=$x_N$]{G}


\Vertex[x=5,y=-2,label=Neuron,size = 1]{E}
\Vertex[x=5,y=-4, label=y]{F}

% Edges
\Edge[bend=-10,Direct=true,color=red,label=\[w_{0=-\theta}\]](A)(E)
\Edge[bend=-5,Direct=true,label=w1](B)(E)
\Edge[bend=-3,Direct=true,label=...](C)(E)
\Edge[bend=5,Direct=true,label=wi](D)(E)
\Edge[bend=5,Direct=true,label=...](D1)(E)
\Edge[bend=5,Direct=true,label=$w_N$](G)(E)
\Edge[bend=0,Direct=true](E)(F)
\end{tikzpicture}
\end{center}
\\
Activation function $f(a)$ can be either \textbf{linear} or \textbf{non-linear}. The popular choices are:
\begin{itemize}
    \item $f(a)=a$ for linear activation function
    \item $f(a)=Sigmoid(a)$ for non-linear activation function
\end{itemize}
\noindent{\color{red} \rule{\linewidth}{0.5mm}}
% ====================================================================================
\section{Least Mean Square (LMS) Learning rules}
\subsection{LMS for Adaline}
 
\begin{definition}[LMSE, LMS error]
For a single neuron Adaline model with one actual output, $y$, and target output, $t$ (also called reference input or desired output), the Least Mean Square Error, $e$, is:
\[error = e = t-y\]
\[E = \frac{1}{2}e^2 =\frac{1}{2} (t-y)^2 \text{    (this is the squared error)}\]
\end{definition}
The learning rule is simply that:
\[\begin{array}{*{20}{c}}
{\Delta {w_i}}& = &{w_i^{new} - w_i^{old}}& = &{w_i^{k + 1} - w_i^k}\\
{\Delta {w_i}}& = &{ - \eta \frac{{\partial E}}{{\partial {w_i}}}}&{where}&{i \in \left[ {0,N} \right]}
\end{array}\]
\textcolor{red}{Where the partial derivative is the gradient of the squared error, the negative sign before $\eta$ means the learning is in the direction of the reduction of squared error in the steepest path.}

\noindent{\color{red} \rule{\linewidth}{0.5mm}}
% ====================================================================================

\section{An interesting derivatives}
It is interesting to proof that for Fermi-Dirac distribution function:
\[y_{F.D} = \frac{1}{{1 + {e^{ - x}}}}\]
The derivative of this function has an very interesting look: 
\[y' = y\left( {1 - y} \right)\]

\noindent{\color{black} \rule{\linewidth}{0.5mm}}
% ====================================================================================
\textbf{PROOF:}
\pushQED{\qed} 
\\
Rewrite $y$ as: 
\[y = \frac{1}{{1 + {e^{ - x}}}} = {\left( {1 + {e^{ - x}}} \right)^{ - 1}}\]
Also, note that:
\[y = \frac{1}{{1 + {e^{ - x}}}} \Rightarrow {e^{ - x}} = \frac{1}{y} - 1\]
Then, it is easy to proof that:
\[\begin{array}{*{20}{c}}
{y'}& = &{ - {{\left( {1 + {e^{ - x}}} \right)}^{ - 2}}\frac{{d\left( {1 + {e^{ - x}}} \right)}}{{dx}}}\\
{}& = &{{{\left( {1 + {e^{ - x}}} \right)}^{ - 2}}\frac{{d\left( {1 + {e^{ - x}}} \right)}}{{d\left( { - x} \right)}}}\\
{}& = &{{{\left( {1 + {e^{ - x}}} \right)}^{ - 2}}{e^{ - x}}}\\
{}& = &{{y^2}\left( {\frac{1}{y} - 1} \right)}\\
{}& = &{y\left( {1 - y} \right)}
\end{array}\]\qedhere

\noindent{\color{black} \rule{\linewidth}{0.5mm}}
% ====================================================================================
For enrichment, let's calculate the derivative for Bose-Einstein distribution function as well:
\[y_{B.E} = \frac{1}{{{e^x} + 1}}\]\pushQED{\qed} 
\textbf{PROOF:}

\[\begin{array}{*{20}{c}}
{y'}& = &{ - {{\left( {1 + {e^x}} \right)}^{ - 2}}\frac{{d\left( {1 + {e^x}} \right)}}{{dx}}}\\
{}& = &{ - {{\left( {1 + {e^x}} \right)}^{ - 2}}\frac{{d\left( {{e^x}} \right)}}{{d\left( x \right)}}}\\
{}& = &{ - {{\left( {1 + {e^x}} \right)}^{ - 2}}{e^x}}\\
{}& = &{ - {y^2}\left( {\frac{1}{y} - 1} \right)}\\
{}& = &{y\left( {y - 1} \right)}
\end{array}\]\qedhere


\noindent{\color{black} \rule{\linewidth}{0.5mm}}
% ====================================================================================
For homework, let's calculate the derivative for Sigmoid function:
\[y = \frac{{{e^x} - {e^{ - x}}}}{{{e^x} + {e^{ - x}}}}\]
\textbf{PROOF:}\\
Rewrite Sigmoid function as:
\[y\left( x \right) = \frac{{{e^x} - {e^{ - x}}}}{{{e^x} + {e^{ - x}}}} = \frac{{{e^x}}}{{{e^x} + {e^{ - x}}}} - \frac{{{e^{ - x}}}}{{{e^x} + {e^{ - x}}}} = \frac{1}{{1 + {e^{ - 2x}}}} - \frac{1}{{1 + {e^{2x}}}} = f\left( x \right) - g\left( x \right)\]
Where:
\[f\left( x \right) = \frac{1}{{1 + {e^{ - 2x}}}}\]
and
\[g\left( x \right) = \frac{1}{{1 + {e^{2x}}}}\]
Also note that:
\[f\left( x \right) + g\left( x \right) = \frac{1}{{1 + {e^{ - 2x}}}} + \frac{1}{{1 + {e^{2x}}}} = \frac{{1 + {e^{ - 2x}} + 1 + {e^{2x}}}}{{1 + {e^{ - 2x}} + 1 + {e^{2x}}}} \equiv 1 \Rightarrow g\left( x \right) = 1 - f\left( x \right)\]
Therefore,
\[\frac{{\partial y\left( x \right)}}{{\partial x}} = \frac{{\partial \left( {f\left( x \right) - g\left( x \right)} \right)}}{{\partial x}} = \frac{{\partial f\left( x \right)}}{{\partial x}} - \frac{{\partial g\left( x \right)}}{{\partial x}}\]
From the derivative of Fermi-Dirac distribution function, we have:
\[\frac{{\partial f\left( x \right)}}{{\partial x}} = 2\frac{{\partial f\left( x \right)}}{{\partial \left( {2x} \right)}} = 2\frac{{\partial \left( {\frac{1}{{1 + {e^{ - 2x}}}}} \right)}}{{\partial \left( {2x} \right)}} = 2f\left( {1 - f} \right)\]
From the derivative of  Bose-Einstein distribution function, we have
\[\frac{{\partial g\left( x \right)}}{{\partial x}} = 2\frac{{\partial g\left( x \right)}}{{\partial \left( {2x} \right)}} = 2\frac{{\partial \left( {\frac{1}{{1 + {e^{2x}}}}} \right)}}{{\partial \left( {2x} \right)}} = 2g\left( {g - 1} \right)\]
Eventually:
\[\frac{{\partial y\left( x \right)}}{{\partial x}} = 2f\left( {1 - f} \right) - 2g\left( {g - 1} \right) = 2fg - 2g\left( { - f} \right) = 4fg\]
